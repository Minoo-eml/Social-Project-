# -*- coding: utf-8 -*-
"""Social research.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10qMQ6R1Oy6SEeqqJQfcnpJxlm_QuQgnX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

stress= pd.read_csv('/content/dreaddit-test.csv')

stress1= pd.read_csv('/content/dreaddit-train.csv')

"""# **Exploration**"""

whole=stress1.append(stress)

whole.head(5)

test = whole.drop(['post_id', 'sentence_range', 'id'], axis = 1)

test.head(2)

whole.shape

whole.info()

whole.describe()

stress.isnull().sum()

whole.columns

whole['subreddit'].unique()

stressfull = whole[whole['label']==1]
notstress = whole[whole['label']==0]

print('total =' , len(whole))
print('people who had stress =' ,len(stressfull) )
print('people who hadnot stress=' ,len(notstress) )

print(whole.subreddit.value_counts())
whole.subreddit.value_counts().plot.barh(color='black')

pd.crosstab(whole["subreddit"] , whole["label"])

whole['sentiment'].hist()

full1 = whole[['text' , 'subreddit']].copy()
full1.head()

import plotly.express as px 
values = whole['subreddit'].value_counts()
labels = whole['subreddit'].value_counts().index

fig = px.pie(whole, names=labels, values=values)
fig.update_layout(title='Distribution of Subreddits', template='plotly_dark')
fig.update_traces(hovertemplate='%{label}: %{value}')
fig.show()

from wordcloud import WordCloud,STOPWORDS

# Start with one review:
text = whole.text[0]

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(' '.join(text))

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

plt.figure(figsize=(18,16))
subset = whole[whole['subreddit']=='ptsd']
text = subset.text.values
cloud1=WordCloud(stopwords=STOPWORDS,background_color='pink',colormap="Dark2",collocations=False,width=2500,height=1800).generate(" ".join(text))

plt.subplot(5,2,1)
plt.axis('off')
plt.title("PTSD",fontsize=10)
plt.imshow(cloud1)

subset = whole[whole['subreddit']=='assistance']
text = subset.text.values
cloud2=WordCloud(stopwords=STOPWORDS,background_color='blue',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,2)
plt.axis('off')
plt.title("Assistance",fontsize=10)
plt.imshow(cloud2)

subset = whole[whole['subreddit']=='relationships']
text = subset.text.values
cloud3=WordCloud(stopwords=STOPWORDS,background_color='gray',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,3)
plt.axis('off')
plt.title("Relationships",fontsize=10)
plt.imshow(cloud3)

subset = whole[whole['subreddit']=='survivorsofabuse']
text = subset.text.values
cloud4=WordCloud(stopwords=STOPWORDS,background_color='black',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,4)
plt.axis('off')
plt.title("Survivors of abuse",fontsize=10)
plt.imshow(cloud4)

subset = whole[whole['subreddit']=='domesticviolence']
text = subset.text.values
cloud5=WordCloud(stopwords=STOPWORDS,background_color='salmon',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,5)
plt.axis('off')
plt.title("Domestic violence",fontsize=10)
plt.imshow(cloud5)


subset = whole[whole['subreddit']=='anxiety']
text = subset.text.values
cloud6=WordCloud(stopwords=STOPWORDS,background_color='red',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,6)
plt.axis('off')
plt.title("Anxiety",fontsize=10)
plt.imshow(cloud6)

subset = whole[whole['subreddit']=='homeless']
text = subset.text.values
cloud7=WordCloud(stopwords=STOPWORDS,background_color='green',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,7)
plt.axis('off')
plt.title("Homeless",fontsize=10)
plt.imshow(cloud7)

subset = whole[whole['subreddit']=='stress']
text = subset.text.values
cloud8=WordCloud(stopwords=STOPWORDS,background_color='yellow',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,8)
plt.axis('off')
plt.title("Stress",fontsize=10)
plt.imshow(cloud8)

subset = whole[whole['subreddit']=='almosthomeless']
text = subset.text.values
cloud9=WordCloud(stopwords=STOPWORDS,background_color='black',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,9)
plt.axis('off')
plt.title("Almost homeless",fontsize=10)
plt.imshow(cloud9)

subset = whole[whole['subreddit']=='food_pantry']
text = subset.text.values
cloud10=WordCloud(stopwords=STOPWORDS,background_color='white',colormap="Dark2",collocations=False,width=2500,height=1800
                       ).generate(" ".join(text))
plt.subplot(5,2,10)
plt.axis('off')
plt.title("Food pantry",fontsize=10)
plt.imshow(cloud10)

# words by label 1
label_1_para = str.lower(''.join(whole[whole['label'] == 1]['text']))
label_1_para[:1000]

stopwords = set(STOPWORDS)
wordcloud = WordCloud(width=1000, height=500, stopwords=stopwords).generate(label_1_para)
plt.figure(figsize=(10,10))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

# words by label =! 1
label_0_para = str.lower(''.join(whole[whole['label'] == 0]['text']))
label_0_para[:1000]

wordcloud = WordCloud(width=1000, height=500, stopwords=stopwords).generate(label_0_para)
plt.figure(figsize=(10,10))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

sentiment_pos = str.lower(''.join(whole[whole['sentiment'] > 0]['text']))
sentiment_pos[:1000]

wordcloud2 = WordCloud(width=1000, height=500, stopwords=stopwords).generate(sentiment_pos)
plt.figure(figsize=(10,10))
plt.imshow(wordcloud2)
plt.axis('off')
plt.show()

sentiment_neg = str.lower(''.join(whole[whole['sentiment'] < 0]['text']))
sentiment_neg[:1000]

wordcloud2 = WordCloud(width=1000, height=500, stopwords=stopwords).generate(sentiment_neg)
plt.figure(figsize=(10,10))
plt.imshow(wordcloud2)
plt.axis('off')
plt.show()

full2 = whole[['text' , 'sentiment']].copy()
full2.head()

full2['feeling'] = whole['label'].map({ 0 : 'Unstressed' , 1 :'Stressed'})
full2.head()

sns.countplot(x='feeling',data = full2, palette= ["#e1c0b6", "#a3b8c8"])

"""# **Machine Learning**"""

x = whole['text']
y= whole['label']

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

vect=CountVectorizer(stop_words="english")
x=vect.fit_transform(x)

x

from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split(x,y , test_size=0.2 , random_state=0)

from sklearn.naive_bayes import MultinomialNB
mb=MultinomialNB()
mb.fit(x_train,y_train)
y_predict = mb.predict(x_test)

from sklearn.metrics import accuracy_score
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_predict,y_test)*100, 2)))

from sklearn.metrics import classification_report
print(classification_report(y_test ,y_predict))
pd.crosstab(y_test , y_predict)

from sklearn.linear_model import LogisticRegression
lo= LogisticRegression(random_state=0)
lo.fit(x_train, y_train)
y_predict = lo.predict(x_test)
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_predict,y_test)*100, 2)))

print(classification_report(y_test ,y_predict))
pd.crosstab(y_test , y_predict)

from sklearn.tree import DecisionTreeClassifier
d=DecisionTreeClassifier()
d.fit(x_train,y_train)
y_predict =d.predict(x_test)
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_test,y_predict)*100, 2)))

from sklearn.metrics import classification_report
print(classification_report(y_test ,y_predict))
pd.crosstab(y_test , y_predict)

from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(x_train,y_train)
y_predict=model.predict(x_test)
print("Model Accuracy is {p}%".format(p =round (accuracy_score(y_test,y_predict)*100, 2)))

from sklearn.metrics import classification_report
print(classification_report(y_test ,y_predict))
pd.crosstab(y_test , y_predict)

"""**testing models performance**"""

def trasactor(a):
  if op == 1:
    print('Unstressed')

  else:
    print('Stressed')

#Using Naive Bayes
prompt = "i am bad"
p = vect.transform([prompt]).toarray()
op = mb.predict(p)
print(trasactor(op))

#Using Naive Bayes
prompt = "i am bad"
p = vect.transform([prompt]).toarray()
op = lo.predict(p)
print(trasactor(op))

#Using Naive Bayes
prompt = "i am bad"
p = vect.transform([prompt]).toarray()
op = d.predict(p)
print(trasactor(op))

#Using Naive Bayes
prompt = "i am bad"
p = vect.transform([prompt]).toarray()
op = model.predict(p)
print(trasactor(op))

#Using Naive Bayes
prompt = "He felt underappreciated and resented his job"
m = vect.transform([prompt]).toarray()
mp = mb.predict(m)
print(trasactor(mp))

#Using Naive Bayes
prompt = "He felt underappreciated and resented his job"
m = vect.transform([prompt]).toarray()
mp = lo.predict(m)
print(trasactor(mp))

#Using Naive Bayes
prompt = "He felt underappreciated and resented his job"
m = vect.transform([prompt]).toarray()
mp = d.predict(m)
print(trasactor(mp))

prompt = "He felt underappreciated and resented his job"
m = vect.transform([prompt]).toarray()
mp = model.predict(m)
print(trasactor(mp))

#Using Logistic Regression
prompt = "The weather is pleasant"
p = vect.transform([prompt]).toarray()
op = d.predict(p)
print(trasactor(mp))

#Using Decision Trees
prompt = "Sometime I feel like I am sad"
a = vect.transform([prompt]).toarray()
ap = lo.predict(a)
print(trasactor(ap))





"""# **NEW DATA**"""

stress2 = pd.read_csv('/content/Stress.csv')

stress2.head(5)

import plotly.express as px
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('wordnet')

import seaborn as sns 
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS

stop_words = stopwords.words('english')
lemmatizer = WordNetLemmatizer()
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2 and token not in stop_words:
            result.append(token)
     
    return result

stress2['clean_text'] = stress2['text'].apply(preprocess)

stress2['clean_text_joined']=stress2['clean_text'].apply(lambda x:" ".join(x))

maxlen = -1
for doc in stress2.clean_text_joined:
    tokens = nltk.word_tokenize(doc)
    if(maxlen<len(tokens)):
        maxlen = len(tokens)
print("The maximum number of words in a text is =", maxlen)
fig = px.histogram(stress2 , x = [len(x) for x in stress2.clean_text], nbins = 50,labels={'label':'Stress Condition'},
                  color=stress2.label)
fig.show()

stressed_word=[]
for words in stress2[stress2.label == 1].clean_text:
    for token in words:
        stressed_word.append(token)
stressed_bigrams_series = (pd.Series(nltk.ngrams(stressed_word, 2)).value_counts())[:20]

stressed_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Bigrams')
plt.ylabel('Bigram')
plt.xlabel('# of Occurances')

non_stressed_word=[]
for words in stress2[stress2.label == 0].clean_text:
    for token in words:
        non_stressed_word.append(token)
non_stressed_bigrams_series = (pd.Series(nltk.ngrams(non_stressed_word, 2)).value_counts())[:20]
non_stressed_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Bigrams')
plt.ylabel('Bigram')
plt.xlabel('# of Occurances')

stressed_word=[]
for words in stress2[stress2.label == 1].clean_text:
    for token in words:
        stressed_word.append(token)
stressed_bigrams_series = (pd.Series(nltk.ngrams(stressed_word, 3)).value_counts())[:20]
stressed_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring trigrams')
plt.ylabel('trigram')
plt.xlabel('# of Occurances')

non_stressed_word=[]
for words in stress2[stress2.label == 0].clean_text:
    for token in words:
        non_stressed_word.append(token)
non_stressed_bigrams_series = (pd.Series(nltk.ngrams(non_stressed_word, 3)).value_counts())[:20]
non_stressed_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))
plt.title('20 Most Frequently Occuring Trigrams')
plt.ylabel('Trigram')
plt.xlabel('# of Occurances')

human_stress = pd.read_csv('/content/Stress.csv')

human_stress['len']=[len(text) for text in human_stress['text']]

human_stress['label_in_value']=human_stress['label'].map({0:'No Stress',1:"Stress"})

from datetime import datetime
human_stress['date']=[ datetime.fromtimestamp(value) for value in human_stress['social_timestamp']]

human_stress['day']=[value for value in human_stress['date'].dt.day]
human_stress['month']=[value for value in human_stress['date'].dt.month]
human_stress['year']=[value for value in human_stress['date'].dt.year]
human_stress['hour']=[value for value in human_stress['date'].dt.hour]
human_stress['second']=[value for value in human_stress['date'].dt.second]
human_stress['day_name']=[value for value in human_stress['date'].dt.day_name()]
human_stress['day_of_week']=[value for value in human_stress['date'].dt.day_of_week]

human_stress.head(10)

remove_cols=['post_id','sentence_range','confidence','social_timestamp','date']
human_stress.drop(remove_cols,axis=1,inplace=True)

human_stress.groupby('subreddit')['len'].describe().sort_values('count',ascending=False)

human_stress.hist(column='len',by='label_in_value',bins=50)
plt.show()

fig, ax = plt.subplots()
human_stress.hist(column='len',by='day_name',bins=50,ax=ax)
plt.subplots_adjust(hspace=1)
plt.show()

sns.lineplot(data=human_stress,x='hour',y='len',hue='label_in_value')
plt.show()

sns.scatterplot(data=human_stress,x='hour',y='len',hue='label_in_value')
plt.show()

sns.catplot(data=human_stress,x='year',y='len',col='label_in_value',sharex=False,col_wrap=3)
plt.show()

sns.catplot(data=human_stress,x='label_in_value',y='len',col='subreddit',col_wrap=3,hue='label_in_value',sharex=False)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

import nltk
import re
from urllib.parse import urlparse
from spacy import load
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('omw-1.4') # Open Multilingual Wordnet, this is an lexical database 
nltk.download('wordnet') 
nltk.download('wordnet2022')
nltk.download('punkt')
nltk.download('stopwords')
! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet

lemmatizer = WordNetLemmatizer()
stop_words = list(stopwords.words('english'))
print(stop_words)

def textPocess(sent):
    try:
        # brackets replacing by space
        sent = re.sub('[][)(]',' ',sent)

        # url removing
        sent = [word for word in sent.split() if not urlparse(word).scheme]
        sent = ' '.join(sent)

        # removing escap characters
        sent = re.sub(r'\@\w+','',sent)

        # removing html tags 
        sent = re.sub(re.compile("<.*?>"),'',sent)

        # getting only characters and numbers from text
        sent = re.sub("[^A-Za-z0-9]",' ',sent)

        # lower case all words
        sent = sent.lower()
        
        # strip all words from sentences
        sent = [word.strip() for word in sent.split()]
        sent = ' '.join(sent)

        # word tokenization
        tokens = word_tokenize(sent)
        
        # removing words which are in stopwords
        for word in tokens:
            if word in stop_words:
                tokens.remove(word)
        
        # lemmatization
        sent = [lemmatizer.lemmatize(word) for word in tokens]
        sent = ' '.join(sent)
        return sent
    
    except Exception as ex:
        print(sent,"\n")
        print("Error ",ex)

stress2['processed_text'] = stress2['text'].apply(lambda text: textPocess(text))
stress2.sample(3)

print("without process ----> ",stress2['text'].iloc[23],end='\n\n')
print("after process ----> ",stress2['processed_text'].iloc[23])

from sklearn.feature_extraction.text import CountVectorizer
MIN_DF = 1

cv = CountVectorizer(min_df=MIN_DF)
cv_df = cv.fit_transform(stress2['processed_text'])
cv_df.toarray()

cv_df = pd.DataFrame(cv_df.toarray(),columns=cv.get_feature_names_out())
cv_df.head(3)



X_train,X_test,y_train,y_test = train_test_split(cv_df,stress2['label'],stratify=stress2['label'])
X_train.shape,y_test.shape

model_lr = LogisticRegression().fit(X_train,y_train)
model_lr.score(X_train,y_train),model_lr.score(X_test,y_test)

def predictor(text):
    processed = textPocess(text)
    embedded_words = cv.transform([text])
    res = model_lr.predict(embedded_words)
    if res[0] == 1:
        res = "this person is in stress"
    else:
        res = "this person is not in stress"
    return res

text1 = """This is the worst thing that happened to me today. I got less marks in my exam, 
            so it is not going to help me in my future."""
text2 = """Hi Shashank sir, I gained a lot of knowledge from you for my future use. 
            This was a very fun journey for me. Thanks for boosting my confidence."""

text3 = """ I am very good today and i feel it help me to control my life"""

print(predictor(text1))
print(predictor(text2))
print(predictor(text2))